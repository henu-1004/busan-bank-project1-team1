# ================================================
#   analyzer_v3.py
#   MULTI-RISK ê²€ì‚¬ + ë¬¸ë§¥ ê¸°ë°˜ Danger/Safe ë¶„ì„ê¸°
# ================================================

import os
import json
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)


# -------------------------------------------------------
# 1) TXT ë¡œë”©
# -------------------------------------------------------
def load_lines(txt_path: str):
    if not os.path.exists(txt_path):
        raise FileNotFoundError(f"TXT íŒŒì¼ ì—†ìŒ: {txt_path}")

    print(f"ğŸ“„ TXT ë¡œë”©: {txt_path}")

    with open(txt_path, "r", encoding="utf-8") as f:
        lines = [line.rstrip("\n") for line in f]

    print(f"ğŸ“Œ ì´ {len(lines)}ê°œ ë¼ì¸ ë¡œë“œë¨")
    return lines


# -------------------------------------------------------
# 2) ë¼ì¸, ê¸€ì ìˆ˜ ê¸°ë°˜ ì²­í‚¹
# -------------------------------------------------------
def chunk_by_lines(lines, max_chars=1200, overlap_lines=9):
    chunks = []
    current = []
    cur_len = 0

    for line in lines:
        line_len = len(line)

        if cur_len + line_len > max_chars:
            chunks.append("\n".join(current))
            current = current[-overlap_lines:] if overlap_lines > 0 else []
            cur_len = sum(len(l) for l in current)

        current.append(line)
        cur_len += line_len

    if current:
        chunks.append("\n".join(current))

    print(f"ğŸ”— ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    return chunks

def chunk_by_chars(text, max_chars=1200, overlap_chars=400):
    chunks = []
    text_len = len(text)

    if overlap_chars >= max_chars:
        raise ValueError("overlap_chars must be smaller than max_chars")

    start = 0

    while start < text_len:
        end = min(start + max_chars, text_len)
        chunk = text[start:end]
        chunks.append(chunk)

        # ë‹¤ìŒ startëŠ” ë°˜ë“œì‹œ ì¦ê°€í•˜ë„ë¡ ë³´ì¥í•´ì•¼ í•œë‹¤
        new_start = end - overlap_chars

        if new_start <= start:  # ë¬´í•œë£¨í”„ ë°©ì§€
            start = end
        else:
            start = new_start

    return chunks




# -------------------------------------------------------
# 3) ìœ„í—˜ ê°€ëŠ¥ ë¬¸êµ¬ í”„ë¦¬í”Œë˜ê·¸
# -------------------------------------------------------
def extract_risky_phrases(chunk: str):
    prompt = f"""
ë„ˆëŠ” ê¸ˆìœµìƒí’ˆ ì„¤ëª… ë¬¸êµ¬ì˜ ê·œì œ ìœ„í—˜ì„±ì„ íŒë‹¨í•˜ëŠ” ì‹¬ì‚¬ê´€ì´ë‹¤.
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ 'ì†Œë¹„ìë³´í˜¸ ê·œì œìƒ ìœ„í—˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë¬¸êµ¬'ë§Œ ì¶”ì¶œí•˜ë¼. (ì¶”ì¸¡Â·ê³¼ì‰ í•´ì„ ê¸ˆì§€. ë¬¸êµ¬ì™€ ë¬¸ë§¥ ìì²´ë§Œ í‰ê°€í•œë‹¤.)
ì•„ë˜ ê¸°ì¤€ì— í•´ë‹¹í•˜ëŠ” ë¬¸êµ¬ë§Œ ë½‘ì•„ë¼:
- ì†Œë¹„ìì˜ ì²­ì•½ì² íšŒê¶ŒÂ·í•´ì§€ê¶ŒÂ·ìë£Œì—´ëŒê¶Œ ë“± ê¶Œë¦¬ë¥¼ í¬ê¸°Â·ì œí•œí•˜ë„ë¡ ìœ ë„
- ì´í•´í•˜ì§€ ëª»í•´ë„ ê°€ì… ê°€ëŠ¥
- ì†ì‹¤ì´ ì ˆëŒ€ ì—†ë‹¤, ë¬´ì¡°ê±´ ë³´ì¥
- ìœ„í—˜ ì—†ìŒÂ·í™•ì • ìˆ˜ìµ ë“± ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ í‘œí˜„
- ì •ë³´ë¥¼ ìˆ¨ê¸°ê±°ë‚˜ ì€íÂ·ëˆ„ë½í•œë‹¤ê³  ì§ì ‘ í‘œí˜„ë¨
- ì†ì‹¤ ì—†ìŒÂ·í™•ì • ìˆ˜ìµ ë“± ì†Œë¹„ì ì˜¤ì¸ ìœ ë°œ ê°€ëŠ¥ í‘œí˜„
- ì†Œë¹„ìì—ê²Œ ë¶ˆì´ìµì„ ê°•ìš”Â·ê¸°ë§Â·ì†ì„ìœ¼ë¡œ ìœ ë„í•˜ëŠ” í‘œí˜„
ë„ˆë¬´ ë„“ê²Œ ì¡ì§€ ë§ê³ , ì§„ì§œ ìœ„í—˜ë¬¸êµ¬ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë½‘ì•„ë¼.

â€» ì•„ë˜ ìœ í˜•ì€ ì •ìƒì ì¸ ë²•ì  ì˜ë¬´ ê³ ì§€ì´ë©° ì ˆëŒ€ ìœ„í—˜ë¬¸êµ¬ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠëŠ”ë‹¤:
- ì •ìƒì  ì ˆì°¨ ì•ˆë‚´
- ìˆ˜ìˆ˜ë£Œ, í™˜ìœ¨ ë³€ë™, ìœ„í—˜ ì•ˆë‚´ ë“± í•„ìˆ˜ ê³ ì§€
- ë‹¨ìˆœí•œ ì„¤ëª…/ì£¼ì˜ë¬¸êµ¬
- ì••ë¥˜, ê°€ì••ë¥˜, ì§ˆê¶Œì„¤ì • ë“±ì— ë”°ë¥¸ ì§€ê¸‰ ì œí•œ ì•ˆë‚´
- â€œ~ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤â€, â€œ~ë¶ˆê°€í•©ë‹ˆë‹¤â€, â€œ~ì ìš©ë©ë‹ˆë‹¤â€ í˜•íƒœì˜ ì ˆì°¨/ë²•ë ¹ ê³ ì§€
- ì˜ˆê¸ˆìë³´í˜¸ ì•ˆë‚´, í™˜ìœ¨ ì•ˆë‚´, ë¦¬ìŠ¤í¬ ì•ˆë‚´ ë“± í•„ìˆ˜ ê³ ì§€

JSONë§Œ ì¶œë ¥:
- ì ˆëŒ€ ë°±í‹±(```) ë˜ëŠ” markdown ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ë§ì•„ë¼.
- JSONì€ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ë¼.

ì¶œë ¥(JSON):
{{
  "phrases": ["...", "..."]
}}

----------------------------------------
[ë¶„ì„ëŒ€ìƒ]
{chunk}
"""
    res = llm.invoke(prompt).content
    try:
        data = json.loads(res)
        return data.get("phrases", [])
    except:
        return []


# -------------------------------------------------------
# 4) ê·œì • RAG ë¦¬ë­í¬
# -------------------------------------------------------
def rerank_rules(query: str, docs: list):
    pairs = []

    for d in docs:
        prompt = f"""
ë„ˆëŠ” ê¸ˆìœµ ê·œì • ë¬¸ì„œ ë¦¬ë­ì»¤ì´ë‹¤.

[ë¶„ì„ ëŒ€ìƒ]
{query}

[í›„ë³´ ê·œì •]
{d.page_content}

ì´ ê·œì •ì´ ë¶„ì„ ëŒ€ìƒê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€
0~100 ì ìˆ˜ë¡œ í‰ê°€í•˜ë¼.

ë°˜ë“œì‹œ "score: XX" í˜•íƒœë¡œë§Œ ì¶œë ¥.
"""
        score_txt = llm.invoke(prompt).content.strip()
        try:
            score = float(score_txt.replace("score:", "").strip())
        except:
            score = 0.0

        pairs.append((score, d.page_content))

    pairs.sort(key=lambda x: x[0], reverse=True)
    return [p[1] for p in pairs]


# -------------------------------------------------------
# 5) Danger/Safe íŒë‹¨ (ë¬¸êµ¬ + ë¬¸ë§¥ + ê·œì •)
# -------------------------------------------------------
def check_violation(phrase: str, chunk_text: str, related_rules: str):
    prompt = f"""
ë„ˆëŠ” ê¸ˆìœµìƒí’ˆ ì„¤ëª… ë¬¸êµ¬ì˜ ê·œì œ ìœ„í—˜ì„±ì„ íŒë‹¨í•˜ëŠ” ì‹¬ì‚¬ê´€ì´ë‹¤.

ì´ ë¬¸ì¥ì€ Danger ë˜ëŠ” Safe ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•œë‹¤.
ì¶”ì¸¡Â·ê³¼ì‰ í•´ì„ ê¸ˆì§€. ë¬¸êµ¬ì™€ ë¬¸ë§¥ ìì²´ë§Œ í‰ê°€í•œë‹¤.

-----------------------------------------
[ Danger â€” ì•„ë˜ ìƒí™©ì´ ì§ì ‘ ëª…ì‹œëœ ê²½ìš°ë§Œ ]
-----------------------------------------
- ì†Œë¹„ìì˜ ì²­ì•½ì² íšŒê¶ŒÂ·í•´ì§€ê¶ŒÂ·ìë£Œì—´ëŒê¶Œ ë“± ê¶Œë¦¬ë¥¼ í¬ê¸°Â·ì œí•œí•˜ë„ë¡ ìœ ë„
- ì •ë³´ë¥¼ ìˆ¨ê¸°ê±°ë‚˜ ì€íÂ·ëˆ„ë½í•œë‹¤ê³  ì§ì ‘ í‘œí˜„ë¨
- ì‚¬ì‹¤ê³¼ ë‹¤ë¦„ì„ ì§ì ‘ì ìœ¼ë¡œ ë§í•¨
- ì†ì‹¤ ì—†ìŒÂ·í™•ì • ìˆ˜ìµ ë“± ì†Œë¹„ì ì˜¤ì¸ ìœ ë°œ ê°€ëŠ¥ í‘œí˜„
- ì†Œë¹„ìì—ê²Œ ë¶ˆì´ìµì„ ê°•ìš”Â·ê¸°ë§Â·ì†ì„ìœ¼ë¡œ ìœ ë„í•˜ëŠ” í‘œí˜„

â€» ì•„ë˜ëŠ” Danger ì•„ë‹˜(Safe) [ì•„ë˜ ìœ í˜•ì€ ì ˆëŒ€ ê¸ˆì†Œë²• ìœ„ë°˜ ì•„ë‹˜ ]:
- ì •ìƒì  ì ˆì°¨ ì•ˆë‚´
- "ì„¤ëª…ë‚´ìš©ì„ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•˜ì˜€ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì„œëª…í•˜ë©´ ê¶Œë¦¬êµ¬ì œê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤" ë“±
  ì†Œë¹„ìì—ê²Œ ì£¼ì˜ë¥¼ ì£¼ëŠ” ì•ˆë‚´ë¬¸ (ì£¼ì˜Â·ê²½ê³  ëª©ì ì˜ ë¬¸ì¥)
- ìˆ˜ìˆ˜ë£Œ, í™˜ìœ¨ ë³€ë™, ìœ„í—˜ ì•ˆë‚´ ë“± í•„ìˆ˜ ê³ ì§€
- ë‹¨ìˆœí•œ ì„¤ëª…/ì£¼ì˜ë¬¸êµ¬(- "~ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤"ëŠ” ì†Œë¹„ì ë³´í˜¸ ì•ˆë‚´ë¬¸ì¼ ë¿, ìœ„ë²• ìœ ë„ ì•„ë‹˜)
- ë²•ì  ì˜ë¬´ì‚¬í•­ ê³ ì§€(fact-based legal notice)
- ì••ë¥˜/ê°€ì••ë¥˜/ì§ˆê¶Œì„¤ì • ë“± ì§€ê¸‰ ì œí•œ ì•ˆë‚´
- "~ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~ë¶ˆê°€í•©ë‹ˆë‹¤", "~ì ìš©ë©ë‹ˆë‹¤" í˜•íƒœì˜ ì •ìƒ ì ˆì°¨ ì•ˆë‚´
- ì˜ˆê¸ˆìë³´í˜¸, ìœ„í—˜ ì•ˆë‚´, í™˜ìœ¨/ìˆ˜ìˆ˜ë£Œ, ì¤‘ë„í•´ì§€ ë¶ˆì´ìµ ê³ ì§€ ë“± ê·œì œê°€ ìš”êµ¬í•˜ëŠ” í•„ìˆ˜ ì„¤ëª…
- ì†Œë¹„ì ì˜¤ì¸ì„ ìœ ë°œí•˜ì§€ ì•ŠëŠ” factual ì•ˆë‚´

JSONë§Œ ì¶œë ¥:
- ì ˆëŒ€ ë°±í‹±(```) ë˜ëŠ” markdown ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ë§ì•„ë¼.
- JSONì€ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ë¼.
-----------------------------------------
ì¶œë ¥(JSONë§Œ):
{{
  "violation": true/false,
  "safe": true/false,
  "rule": "ê´€ë ¨ ê·œì • ë˜ëŠ” null",
  "reason": "í•µì‹¬ ì´ìœ ",
  "risky_words": []
}}

-----------------------------------------
ë…¼ì˜ ì—¬ì§€ëŠ” ì°¸ê³ ë§Œ í•˜ê³   
íŒë‹¨ì€ ë°˜ë“œì‹œ ë¬¸ë§¥ ì „ì²´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë¼

[ë…¼ì˜ ì—¬ì§€]
{phrase}

[ë¬¸ë§¥ ì „ì²´]
{chunk_text}

[ê´€ë ¨ ê·œì •]
{related_rules}
"""
    return llm.invoke(prompt).content


# -------------------------------------------------------
# 6) ì²­í¬ ê¸°ë°˜ ë©€í‹° ë¶„ì„ (ë¦¬ë­ì»¤ ì˜µì…˜ ì¶”ê°€)
# -------------------------------------------------------
def analyze_chunk(chunk, retriever, rules_per_phrase=5, use_reranker=False):
    risky_list = extract_risky_phrases(chunk)

    # ìœ„í—˜ ë¬¸êµ¬ ì—†ìœ¼ë©´ chunk ì „ì²´ë¥¼ ë¶„ì„
    if len(risky_list) == 0:
        risky_list = [chunk]

    results = []

    for phrase in risky_list:
        # 1) FAISS Top-k ê²€ìƒ‰
        retrieved_docs = retriever.invoke(phrase)[:rules_per_phrase]

        # 2) ë¦¬ë­ì»¤ ì˜µì…˜ ì ìš© ì—¬ë¶€
        if use_reranker and retrieved_docs:
            ranked_rules = rerank_rules(phrase, retrieved_docs)
            all_rules_text = "\n".join(ranked_rules)
            first_rule_line = ranked_rules[0].split("\n")[0].strip()
        else:
            if retrieved_docs:
                all_rules_text = "\n".join([d.page_content for d in retrieved_docs])
                first_rule_line = retrieved_docs[0].page_content.split("\n")[0].strip()
            else:
                all_rules_text = "ê´€ë ¨ ê·œì • ì—†ìŒ"
                first_rule_line = "ê´€ë ¨ ê·œì • ì—†ìŒ"

        # 3) Danger/Safe íŒì •
        violation_json = check_violation(
            phrase=phrase,
            chunk_text=chunk,
            related_rules=all_rules_text
        )

        results.append({
            "phrase": phrase,
            "rule_first_line": first_rule_line,
            "violation": violation_json
        })

    return results





# -------------------------------------------------------
# 7) ì „ì²´ TXT ë¶„ì„ (ë¦¬ë­ì»¤ ì˜µì…˜ ì „ë‹¬)
# -------------------------------------------------------
def analyze(lines, retriever, use_reranker=False):
    full_text = "\n".join(lines)
    chunks = chunk_by_chars(full_text, max_chars=1000, overlap_chars=300)
    results = []

    print(f"ğŸ” ì´ {len(chunks)}ê°œ ì²­í¬ ë¶„ì„ ì‹œì‘")

    for idx, chunk in enumerate(chunks):
        print(f"\nğŸ“Œ ì²­í¬ ë¶„ì„: {idx+1}/{len(chunks)}")

        # ì˜µì…˜ ë„˜ê¸°ê¸°
        chunk_result = analyze_chunk(
            chunk,
            retriever,
            use_reranker=use_reranker
        )

        results.append({
            "chunk_index": idx,
            "chunk": chunk,
            "details": chunk_result
        })

    return results



# -------------------------------------------------------
# 8) ìµœì¢… AI ì½”ë©˜íŠ¸ ìƒì„±
# -------------------------------------------------------
def generate_ai_comment(all_results):
    dangers = []
    safes = []

    for chunk_info in all_results:
        for d in chunk_info["details"]:
            try:
                v = json.loads(d["violation"])
            except:
                continue

            if v.get("violation"):
                dangers.append({
                    "chunk_index": chunk_info["chunk_index"],
                    "phrase": d["phrase"],
                    "details": v
                })
            else:
                safes.append({
                    "chunk_index": chunk_info["chunk_index"],
                    "phrase": d["phrase"],
                    "details": v
                })

    overall = "Danger" if len(dangers) > 0 else "Safe"

    prompt = f"""
ë‹¤ìŒì€ ìœ„í—˜ ë¶„ì„ ê²°ê³¼ì´ë‹¤.

[Danger]
{json.dumps(dangers, ensure_ascii=False, indent=2)}

[Safe]
{json.dumps(safes, ensure_ascii=False, indent=2)}

ì „ì²´ ë“±ê¸‰: {overall}

-----------------------------------------------
ì½”ë©˜íŠ¸ ì‘ì„± ê·œì¹™
-----------------------------------------------
- Danger ì¡´ì¬: ìœ„í—˜ ë¬¸êµ¬ ì¸ìš© + ìœ„ë°˜ ì´ìœ  + ê°œì„  ì œì•ˆ
- Safeë§Œ ì¡´ì¬: â€œìœ„ë°˜ ë˜ëŠ” ì˜¤í•´ ê°€ëŠ¥ì„±ì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.â€ í¬í•¨
- Danger ì—†ì„ ê²½ìš° Danger í•­ëª© ì œê±°
- Safe í•­ëª© 1ê°œ ì´ìƒ í¬í•¨

JSONë§Œ ì¶œë ¥:
- ì ˆëŒ€ ë°±í‹±(```) ë˜ëŠ” markdown ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ë§ì•„ë¼.
- JSONì€ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ë¼.

JSONë§Œ ì¶œë ¥:
{{
  "overall_risk": "{overall}",
  "comments": []
}}
"""

    final = llm.invoke(prompt).content

    return {
        "llm_comment": final,
        "overall": overall,
        "danger_count": len(dangers)
    }


# =====================================================================
# â­â­ FastAPIê°€ í˜¸ì¶œí•˜ëŠ” í†µí•© ë¶„ì„ í•¨ìˆ˜ â­â­
# =====================================================================
def run_ai_risk_analysis(pdf_id: int, txt_path: str, save_base_dir: str, original_file_name: str):

    print("ğŸ“š ê·œì • ë²¡í„°DB ë¡œë“œ ì¤‘...")
    FAISS_PATH = "/home/g5223sho/bnk_ai_server/pdf_Ai/pdf_comment/test/faiss_db"

    embed = OpenAIEmbeddings(model="text-embedding-3-large")
    db = FAISS.load_local(FAISS_PATH, embed, allow_dangerous_deserialization=True)    
    retriever = db.as_retriever(search_kwargs={"k": 7})

    # 1) TXT ë¡œë“œ
    lines = load_lines(txt_path)

    # 2) ìœ„í—˜ ë¶„ì„
    results = analyze(lines, retriever, use_reranker=True)#False

    # ì €ì¥ ë””ë ‰í† ë¦¬
    save_dir = os.path.join(save_base_dir, str(pdf_id))
    os.makedirs(save_dir, exist_ok=True)

    # 3) ë””í…Œì¼ TXT ì €ì¥
    detail_filename = f"{original_file_name}_analyze_detail.txt"
    detail_path = os.path.join(save_dir, detail_filename)
    with open(detail_path, "w", encoding="utf-8") as f:
        for chunk_info in results:
            f.write(f"=== Chunk {chunk_info['chunk_index']} ===\n")
            for d in chunk_info["details"]:
                f.write(f"- ë¬¸êµ¬: {d['phrase']}\n")
                f.write(f"- ê´€ë ¨ ê·œì •(1ì¤„): {d['rule_first_line']}\n")
                f.write(f"- ë¶„ì„ê²°ê³¼ JSON: {d['violation']}\n")
                f.write("\n")

    # 4) íŒŒì´ë„ ì½”ë©˜íŠ¸ ìƒì„±
    final_comment = generate_ai_comment(results)

    # 5) íŒŒì´ë„ ì½”ë©˜íŠ¸ TXT ì €ì¥
    final_filename = f"{original_file_name}_final_comment.txt"
    final_path = os.path.join(save_dir, final_filename)
    with open(final_path, "w", encoding="utf-8") as f:
        f.write(final_comment["llm_comment"])

    print(f"ğŸ ë¶„ì„ ì™„ë£Œ â†’ PDF_ID={pdf_id}")

    # ğŸ”¥ DB ì—…ë°ì´íŠ¸ëŠ” main.pyì—ì„œ ìˆ˜í–‰í•˜ë¯€ë¡œ ë°˜í™˜ë§Œ í•œë‹¤
    return {
        "overall": final_comment["overall"],
        "danger_count": final_comment["danger_count"],
        "detail_path": detail_path,
        "final_comment_path": final_path
    }
