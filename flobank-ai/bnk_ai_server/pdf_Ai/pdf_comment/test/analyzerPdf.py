import os
import json
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)


# -------------------------------------------------------
# 1) TXT ë¡œë”©
# -------------------------------------------------------
def load_lines(txt_path: str):
    if not os.path.exists(txt_path):
        raise FileNotFoundError(f"TXT íŒŒì¼ ì—†ìŒ: {txt_path}")
    print(f"ğŸ“„ TXT ë¡œë”©: {txt_path}")

    with open(txt_path, "r", encoding="utf-8") as f:
        lines = [line.rstrip("\n") for line in f]

    print(f"ğŸ“Œ ì´ {len(lines)}ê°œ ë¼ì¸ ë¡œë“œë¨")
    return lines


# -------------------------------------------------------
# 2) ë¼ì¸ ê¸°ë°˜ ì²­í‚¹
# -------------------------------------------------------
def chunk_by_lines(lines, max_chars=1200, overlap_lines=9):
    chunks = []
    current = []
    cur_len = 0

    for line in lines:
        line_len = len(line)

        if cur_len + line_len > max_chars:
            chunks.append("\n".join(current))
            current = current[-overlap_lines:] if overlap_lines > 0 else []
            cur_len = sum(len(l) for l in current)

        current.append(line)
        cur_len += line_len

    if current:
        chunks.append("\n".join(current))

    print(f"ğŸ”— ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    return chunks


# -------------------------------------------------------
# 3) ê·œì • ë¦¬ë­í¬
# -------------------------------------------------------
def rerank_rules(query: str, docs: list):
    pairs = []

    for d in docs:
        prompt = f"""
ë„ˆëŠ” ê¸ˆìœµ ê·œì • ë¬¸ì„œ ë¦¬ë­ì»¤ì´ë‹¤.

[ë¶„ì„ ëŒ€ìƒ]
{query}

[í›„ë³´ ê·œì •]
{d.page_content}

ì´ ê·œì •ì´ ë¶„ì„ ëŒ€ìƒê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€
0~100 ì ìˆ˜ë¡œ í‰ê°€í•˜ë¼.

ë°˜ë“œì‹œ "score: XX" í˜•íƒœë¡œë§Œ ì¶œë ¥.
"""
        score_txt = llm.invoke(prompt).content.strip()
        try:
            score = float(score_txt.replace("score:", "").strip())
        except:
            score = 0.0

        pairs.append((score, d.page_content))

    pairs.sort(key=lambda x: x[0], reverse=True)
    return [p[1] for p in pairs]


# -------------------------------------------------------
# 4) ìœ„ê·œ íŒë‹¨ (Danger / Safe 2ë‹¨ê³„)
# -------------------------------------------------------
def check_violation(chunk: str, related_rules: str):
    prompt = f"""
ë„ˆëŠ” ê¸ˆìœµìƒí’ˆ ì„¤ëª… ë¬¸êµ¬ì˜ ê·œì œ ìœ„í—˜ì„±ì„ íŒë‹¨í•˜ëŠ” ì‹¬ì‚¬ê´€ì´ë‹¤.

ì´ ë¬¸ì¥ì€ Danger ë˜ëŠ” Safe ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•œë‹¤.
ì¶”ì¸¡Â·ê³¼ì‰ í•´ì„ ê¸ˆì§€. ë¬¸êµ¬ ìì²´ë§Œ í‰ê°€í•œë‹¤.

-----------------------------------------
[ Danger â€” ì•„ë˜ ìƒí™©ì´ ì§ì ‘ ëª…ì‹œëœ ê²½ìš°ë§Œ ]
-----------------------------------------
- ì†Œë¹„ìì˜ ì²­ì•½ì² íšŒê¶ŒÂ·í•´ì§€ê¶ŒÂ·ìë£Œì—´ëŒê¶Œ ë“± ë²•ì  ê¶Œë¦¬ë¥¼ í¬ê¸°í•˜ë„ë¡ ìœ ë„í•˜ê±°ë‚˜ ì œí•œí•œë‹¤ê³  ì§ì ‘ ëª…ì‹œí•¨
- ì •ë³´ë¥¼ ìˆ¨ê¸°ê±°ë‚˜ ì€íÂ·ëˆ„ë½í•œë‹¤ê³  ì§ì ‘ í‘œí˜„ë¨
- ì‚¬ì‹¤ê³¼ ë‹¤ë¦„ì„ ì§ì ‘ì ìœ¼ë¡œ ë§í•¨
- ì†Œë¹„ìì—ê²Œ ë¶ˆì´ìµì„ ê°•ìš”Â·ê¸°ë§Â·ì†ì„ìœ¼ë¡œ ìœ ë„í•˜ëŠ” í‘œí˜„ì´ ì§ì ‘ ë“±ì¥

â€» ë‹¨, ì•„ë˜ëŠ” Danger ì•„ë‹˜ (ë¬´ì¡°ê±´ Safe):
- í™˜ìœ¨ë³€ë™Â·ìˆ˜ìˆ˜ë£ŒÂ·ì´ìœ¨Â·í™˜ì°¨ì† ë“± ì™¸í™”ì˜ˆê¸ˆ í•„ìˆ˜ ì•ˆë‚´ ë¬¸êµ¬
- ì••ë¥˜, íœ´ë©´ì˜ˆê¸ˆ, í˜„ì°°ìˆ˜ìˆ˜ë£Œ ë“± ì€í–‰ì´ ë°˜ë“œì‹œ ì•ˆë‚´í•´ì•¼ í•˜ëŠ” ì˜ë¬´ ê³ ì§€
- ì ˆì°¨ ì•ˆë‚´ ë˜ëŠ” ì •ìƒì ì¸ ì œí•œ ì„¤ëª…
- ìƒí’ˆ ì´í•´ë¥¼ ìœ„í•œ ì¼ë°˜ì  ì£¼ì˜ ë¬¸êµ¬


-----------------------------------------
[ Safe ]
-----------------------------------------
ìœ„ Danger ê¸°ì¤€ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ëª¨ë“  ë¬¸êµ¬ëŠ” Safe.

=========================================
JSONë§Œ ì¶œë ¥:
- ì ˆëŒ€ ë°±í‹±(```) ë˜ëŠ” markdown ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ë§ì•„ë¼.
- JSONì€ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ë¼.
{{
  "violation": true/false,
  "safe": true/false,
  "rule": "ê´€ë ¨ ê·œì • ë˜ëŠ” null",
  "reason": "í•µì‹¬ ê·¼ê±°",
  "risky_words": []
}}

=========================================
[ë¶„ì„ëŒ€ìƒ]
{chunk}

[ê´€ë ¨ ê·œì •]
{related_rules}

"""

    return llm.invoke(prompt).content


# -------------------------------------------------------
# 5) ì „ì²´ TXT ë¶„ì„
# -------------------------------------------------------
def analyze(lines, retriever, rules_per_chunk=10, top_k_after_rerank=5):
    chunks = chunk_by_lines(lines)
    print(f"ğŸ” ì´ {len(chunks)}ê°œ ì²­í¬ ë¶„ì„ ì‹œì‘")

    results = []

    for idx, chunk in enumerate(chunks):
        print(f"\nğŸ“Œ ì²­í¬ ë¶„ì„ ì¤‘: {idx+1}/{len(chunks)}")

        retrieved_docs = retriever.invoke(chunk)[:rules_per_chunk]
        reranked = rerank_rules(chunk, retrieved_docs)
        selected_rules = "\n\n".join(reranked[:top_k_after_rerank])
        violation = check_violation(chunk, selected_rules)

        results.append({
            "chunk_index": idx,
            "chunk": chunk,
            "related_rules": selected_rules,
            "violation": violation
        })

    return results


# -------------------------------------------------------
# 6) ì €ì¥
# -------------------------------------------------------
def save_results(result, txt_path):
    base_name = os.path.basename(txt_path).replace(".txt", "")
    out_dir = "analysis_result"
    os.makedirs(out_dir, exist_ok=True)

    json_path = os.path.join(out_dir, f"{base_name}_result.json")
    txt_path2 = os.path.join(out_dir, f"{base_name}_result.txt")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=4)

    with open(txt_path2, "w", encoding="utf-8") as f:
        for r in result:
            f.write(f"[Chunk {r['chunk_index']}]\n")
            f.write("==== Chunk ë‚´ìš© ====\n")
            f.write(r["chunk"] + "\n\n")
            f.write("==== ê´€ë ¨ ê·œì • ====\n")
            f.write(r["related_rules"] + "\n\n")
            f.write("==== ìœ„ê·œ íŒë‹¨ ====\n")
            f.write(r["violation"] + "\n")
            f.write("\n" + "="*80 + "\n\n")

    print(f"ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ â†’ {json_path}")
    print(f"ğŸ’¾ TXT ì €ì¥ ì™„ë£Œ â†’ {txt_path2}")

    return json_path, txt_path2


# -------------------------------------------------------
# 7) ìµœì¢… PDF AI ì½”ë©˜íŠ¸ ìƒì„± (Danger / Safe)
# -------------------------------------------------------
def generate_pdf_comment(results):
    llm_local = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    dangers = []
    safes = []

    for r in results:
        try:
            v = json.loads(r["violation"])
        except:
            continue

        if v.get("violation"):
            dangers.append({
                "chunk_index": r["chunk_index"],
                "chunk": r["chunk"],
                "details": v
            })
        else:
            safes.append({
                "chunk_index": r["chunk_index"],
                "chunk": r["chunk"],
                "details": v
            })

    danger_count = len(dangers)
    safe_count = len(safes)

    overall = "Danger" if danger_count > 0 else "Safe"

    prompt = f"""
ë‹¤ìŒì€ ë¶„ì„ ê²°ê³¼ì´ë‹¤.

[Danger]
{json.dumps(dangers, ensure_ascii=False, indent=2)}

[Safe]
{json.dumps(safes, ensure_ascii=False, indent=2)}

ì „ì²´ ë“±ê¸‰: {overall}

-----------------------------------------------
ì½”ë©˜íŠ¸ ìƒì„± ê·œì¹™
-----------------------------------------------
- Danger ì¡´ì¬: ë¬¸ì œ ë¬¸êµ¬ ì¸ìš© + ë¬¸ì œê°€ ë˜ëŠ” ì´ìœ  + ê°œì„ ë°©í–¥
- Danger ì—†ìŒ(Safeë§Œ ì¡´ì¬):  
  â€œìœ„ë°˜ ë˜ëŠ” ì˜¤í•´ ê°€ëŠ¥ì„±ì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ìœ¼ë©° ì„¤ëª…ì´ ëª…í™•í•©ë‹ˆë‹¤.â€  
  ê°™ì€ ê¸ì • ì½”ë©˜íŠ¸ ìµœì†Œ 1ê°œ í¬í•¨
- ì ˆëŒ€ ë°±í‹±(```) ë˜ëŠ” markdown ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ë§ì•„ë¼.
- JSONì€ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ë¼.

ì¶œë ¥(JSON):
{{
  "overall_risk": "{overall}",
  "comments": [
    {{"type": "Danger", "text": "..."}},
    {{"type": "Safe", "text": "..."}}
  ]
}}

ì£¼ì˜:
- Danger ì—†ìœ¼ë©´ Danger í•­ëª© ë¹¼ê¸°
- Safe í•­ëª©ì€ ìµœì†Œ 1ê°œ í¬í•¨
- JSONë§Œ ì¶œë ¥
"""

    llm_comment = llm_local.invoke(prompt).content

    return {
        "llm_comment": llm_comment,
        "danger_count": danger_count,
        "safe_count": safe_count,
        "overall_risk": overall
    }


# -------------------------------------------------------
# 8) ì‹¤í–‰ë¶€
# -------------------------------------------------------
if __name__ == "__main__":
    print("ğŸ“š ê·œì • ë²¡í„°DB ë¡œë“œ ì¤‘...")

    FAISS_PATH = "/home/g5223sho/bnk_ai_server/pdf_Ai/pdf_comment/test/faiss_db"
    embed = OpenAIEmbeddings(model="text-embedding-3-large")
    db = FAISS.load_local(FAISS_PATH, embed, allow_dangerous_deserialization=True)
    retriever = db.as_retriever(search_kwargs={"k": 15})

    target_txt = "/home/g5223sho/bnk_ai_server/pdf_Ai/pdf_comment/test/pdf_temp/ê°€ì§œì„¤ëª…ì„œ.txt"

    lines = load_lines(target_txt)
    result = analyze(lines, retriever)
    save_results(result, target_txt)

    print("\nğŸ§  PDF AI ìµœì¢… ì½”ë©˜íŠ¸ ìƒì„± ì¤‘...\n")
    final_comment = generate_pdf_comment(result)

    out_path = "analysis_result/final_comment.json"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(final_comment["llm_comment"])

    print(f"ğŸ’¬ PDF AI ì½”ë©˜íŠ¸ ì €ì¥ ì™„ë£Œ â†’ {out_path}")
    print("ğŸ”¢ ìœ„í—˜ ì¹´ìš´íŠ¸:", final_comment["danger_count"])
    print("ğŸ· ìµœì¢… ë“±ê¸‰:", final_comment["overall_risk"])
