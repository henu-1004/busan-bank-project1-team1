# mk_crawler.py
import requests
import time
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from db import insert_article, get_existing_urls

HEADERS = {"User-Agent": "Mozilla/5.0"}


# ---------------------------------------------------
# ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ íŒŒì‹± (ì •í™•í•œ #list_area ì˜ì—­ë§Œ)
# ---------------------------------------------------
def mk_fetch_list(url):
    res = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    results = []

    # ğŸ”¥ ì›í•˜ëŠ” ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸: #list_area ë‚´ë¶€ë§Œ!
    container = soup.select_one("#list_area")
    if not container:
        return [], None

    for a in container.select("a.news_item"):
        title_tag = a.select_one("h3.news_ttl")
        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)

        link = a.get("href")
        if not link.startswith("http"):
            link = "https://www.mk.co.kr" + link

        # ë‚ ì§œ í…ìŠ¤íŠ¸
        date_tag = a.select_one(".time_info")
        date_text = date_tag.get_text(strip=True) if date_tag else ""

        results.append((title, link, date_text))

    # ğŸ”¥ ë‹¤ìŒ í˜ì´ì§€(â€œë”ë³´ê¸°â€) ë²„íŠ¼ â†’ ë™ì  API ê¸°ë°˜
    btn = soup.select_one("button.drop_sub_news_btn")
    next_page = None
    if btn:
        api_input = soup.select_one(btn.get("data-source-selector"))
        if api_input:
            api_value = api_input.get("value")  # //www.mk.co.kr/_CP/42
            if api_value:
                next_page = "https:" + api_value

    return results, next_page


# ---------------------------------------------------
# ë¦¬ìŠ¤íŠ¸ ë‚ ì§œ í•´ì„ (ì ˆëŒ€ â†’ datetime)
# ---------------------------------------------------
def parse_mk_list_date(date_text):
    date_text = date_text.strip()

    # ì ˆëŒ€ì‹œê°„
    for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M"]:
        try:
            return datetime.strptime(date_text, fmt)
        except:
            continue

    # ìƒëŒ€ì‹œê°„ ("2ì‹œê°„ ì „") â†’ ìƒì„¸ í•„ìš”
    if "ì „" in date_text:
        return None

    return None


# ---------------------------------------------------
# ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
# ---------------------------------------------------
def mk_parse_detail(url):
    res = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    # ë‚ ì§œ íŒŒì‹±
    dt_real = None
    dt_tag = soup.select_one("dl.registration dd")
    if dt_tag:
        dt_text = dt_tag.get_text(strip=True)
        for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M"]:
            try:
                dt_real = datetime.strptime(dt_text, fmt)
                break
            except:
                pass

    # ìš”ì•½
    summary_tag = soup.select_one("div.midtitle_text")
    summary = summary_tag.get_text(" ", strip=True) if summary_tag else ""

    # ë³¸ë¬¸ íŒŒì‹±
    parent = soup.select_one("div.news_cnt_detail_wrap")
    if parent:
        content = "\n".join(
            [p.get_text(" ", strip=True) for p in parent.select("p") if p.get_text(strip=True)]
        )
    else:
        content = ""

    return dt_real, summary, content


# ---------------------------------------------------
# MK í¬ë¡¤ëŸ¬ ì‹¤í–‰
# ---------------------------------------------------
def crawl_mk(section, base_url, mode="oneday"):
    now = datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    limit_dt = now - timedelta(days=5)

    existing_urls = get_existing_urls()
    page_url = base_url

    while True:
        page_list, next_page = mk_fetch_list(page_url)

        for title, url, date_text in page_list:

            if url in existing_urls:
                print("â© skip:", title)
                continue

            dt_preview = parse_mk_list_date(date_text)
            need_detail = dt_preview is None

            # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì ˆëŒ€ ì‹œê°„ ì¡´ì¬ + ìµœê·¼5 STOP
            if not need_detail and mode == "recent5":
                if dt_preview < limit_dt:
                    print("â›” MK: ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœê·¼5ì¼ ì´ì „ ê¸°ì‚¬ â†’ STOP")
                    return

            # ìƒì„¸ ìš”ì²­
            print("ğŸ†• MK ìƒì„¸:", title)
            dt_real, summary, content = mk_parse_detail(url)
            if not dt_real:
                continue

            # ìƒì„¸ STOP
            if mode == "oneday":
                if dt_real.strftime("%Y-%m-%d") != today_str:
                    continue
            else:
                if dt_real < limit_dt:
                    print("â›” MK ìƒì„¸: ìµœê·¼5ì¼ ì´ì „ ê¸°ì‚¬ â†’ STOP")
                    return

            insert_article(
                company="MK",
                category=section,
                title=title,
                url=url,
                written_at=dt_real,
                summary=summary,
                content=content,
            )

            existing_urls.add(url)
            time.sleep(0.3)

        # ë‹¤ìŒ í˜ì´ì§€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not next_page:
            break

        page_url = next_page
