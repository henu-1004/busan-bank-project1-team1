# hk_crawler.py
import requests, time, traceback, re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from db import insert_article, get_existing_urls, exists_url


HEADERS = {"User-Agent": "Mozilla/5.0"}

HK_SECTIONS = {
    "macro": "https://www.hankyung.com/economy/macro",
    "forex": "https://www.hankyung.com/economy/forex",
}


# -------------------------------------
# ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡
# -------------------------------------
def log_error(msg):
    with open("hk_error.log", "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now()}] {msg}\n")
        f.write(traceback.format_exc() + "\n\n")


# -------------------------------------
# ë¦¬ìŠ¤íŠ¸ ë‚ ì§œ íŒŒì‹±(1ì°¨ í•„í„°)
# -------------------------------------
def parse_date_preview(date_str):
    try:
        if " " in date_str:
            return datetime.strptime(date_str.strip(), "%Y.%m.%d %H:%M")
        else:
            return datetime.strptime(date_str.strip(), "%Y.%m.%d")
    except:
        return None


# -------------------------------------
# ë³¸ë¬¸ ì •ë¦¬
# -------------------------------------
def clean_hk_content(text):
    text = re.sub(r".*ê¸°ì.*", "", text)
    text = re.sub(r"\S+@hankyung\.com", "", text)
    text = re.sub(r"â“’.*", "", text)
    lines = [line.strip() for line in text.split("\n")]
    return "\n".join([l for l in lines if l])


# -------------------------------------
# ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
# -------------------------------------
def hk_parse_article(url):
    try:
        res = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(res.text, "html.parser")

        # ì…ë ¥ ì‹œê°„
        dt_real = None
        dt_tags = soup.select("div.datetime span.item .txt-date")
        if dt_tags:
            dt_str = dt_tags[0].get_text(strip=True)
            try:
                dt_real = datetime.strptime(dt_str, "%Y.%m.%d %H:%M")
            except:
                pass

        # ìš”ì•½
        summary_tag = soup.select_one("div.summary")
        summary = summary_tag.get_text(strip=True) if summary_tag else ""

        # ë³¸ë¬¸
        body_tag = soup.select_one("div.article-body")
        content = body_tag.get_text("\n", strip=True) if body_tag else ""
        content = clean_hk_content(content)

        return dt_real, summary, content

    except:
        log_error(f"HK ìƒì„¸ ì‹¤íŒ¨: {url}")
        return None, "", ""


# -------------------------------------
# ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìˆ˜ì§‘
# -------------------------------------
def hk_fetch_list(url):
    try:
        res = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(res.text, "html.parser")

        items = []
        for li in soup.select("ul.news-list li"):
            a = li.find("a")
            if not a:
                continue

            title = a.get_text(strip=True)
            link = a["href"]
            if not link.startswith("http"):
                link = "https://www.hankyung.com" + link

            date_tag = li.select_one("p.txt-date")
            if not date_tag:
                continue
            date_text = date_tag.get_text(strip=True)

            items.append((title, link, date_text))

        next_btn = soup.select_one("a.btn-next")
        next_page = next_btn["href"] if next_btn else None
        if next_page and not next_page.startswith("http"):
            next_page = "https://www.hankyung.com" + next_page

        return items, next_page

    except:
        log_error(f"HK LIST ì‹¤íŒ¨: {url}")
        return [], None


# -------------------------------------
# ë©”ì¸ í¬ë¡¤ëŸ¬
# -------------------------------------
def crawl_hk(section, mode="today"):
    base_url = HK_SECTIONS[section]
    now = datetime.now()

    today_str = now.strftime("%Y.%m.%d")
    limit_5d = now - timedelta(days=5)

    existing_urls = get_existing_urls()
    print(f"\nğŸ”¥ ê¸°ì¡´ URL {len(existing_urls)}ê°œ ë¡œë”©ë¨\n")

    page_url = base_url

    while True:

        page_list, next_page = hk_fetch_list(page_url)

        for title, url, date_text in page_list:

            if url in existing_urls:
                print("â© DB ìŠ¤í‚µ:", title)
                continue

            # -------------------------------
            # 1ì°¨ í•„í„° (ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€)
            # -------------------------------
            dt_preview = parse_date_preview(date_text)
            if not dt_preview:
                continue

            if mode == "today":
                if dt_preview.strftime("%Y.%m.%d") != today_str:
                    continue
            else:  # recent5
                if dt_preview < limit_5d:
                    print("â›” HK: ìµœê·¼5ì¼ ì´ì „ ê¸°ì‚¬ â†’ STOP")
                    return

            # -------------------------------
            # ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
            # -------------------------------
            print("ğŸ†• HK ìƒì„¸:", title)
            dt_real, summary, content = hk_parse_article(url)

            if not dt_real:
                continue

            # -------------------------------
            # ìƒì„¸ ë‚ ì§œ ê¸°ì¤€ 2ì°¨ í•„í„° + STOP
            # -------------------------------
            if mode == "today":
                if dt_real.strftime("%Y.%m.%d") != today_str:
                    continue
            else:
                if dt_real < limit_5d:
                    print("â›” HK ìƒì„¸: ìµœê·¼5ì¼ ì´ì „ ê¸°ì‚¬ â†’ STOP")
                    return

            # -------------------------------
            # DB ì €ì¥
            # -------------------------------
            insert_article(
                company="HK",
                category=section,
                title=title,
                url=url,
                written_at=dt_real,
                summary=summary,
                content=content,
            )

            existing_urls.add(url)
            time.sleep(0.2)

        if not next_page:
            break

        page_url = next_page
