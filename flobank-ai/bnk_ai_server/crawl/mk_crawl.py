# ===========================
#        mk_crawler.py
# ===========================
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json, os, time, traceback

HEADERS = {"User-Agent": "Mozilla/5.0"}

# ----------------------------------------
# ê³µí†µ ì—ëŸ¬ ë¡œê·¸
# ----------------------------------------
def log_error(msg):
    with open("error.log", "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now()}] {msg}\n")
        f.write(traceback.format_exc() + "\n\n")


# ----------------------------------------
# JSON ë¡œë“œ
# ----------------------------------------
def load_existing(filename):
    if not os.path.exists(filename):
        return [], set()

    try:
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data, {x["url"] for x in data}
    except:
        log_error(f"MK JSON ë¡œë”© ì‹¤íŒ¨: {filename}")
        return [], set()


# ----------------------------------------
# ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ íŒŒì‹±
# ----------------------------------------
def mk_fetch_list(url):
    try:
        res = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(res.text, "html.parser")

        items = soup.select("a.news_item")
        results = []

        for i in items:
            title_tag = i.select_one("h3.news_ttl")
            if not title_tag:
                continue

            title = title_tag.get_text(strip=True)
            link = i["href"]
            if not link.startswith("http"):
                link = "https://www.mk.co.kr" + link

            results.append((title, link))

        next_btn = soup.select_one("a.btn_next")
        next_page = next_btn["href"] if next_btn else None

        return results, next_page

    except:
        log_error(f"MK ë¦¬ìŠ¤íŠ¸ ì‹¤íŒ¨: {url}")
        return [], None


# ----------------------------------------
# ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
# ----------------------------------------
def mk_parse_article(url):
    try:
        res = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(res.text, "html.parser")

        # ë‚ ì§œ
        dt_tag = soup.select_one("dl.registration dd")
        if dt_tag:
            t = dt_tag.get_text(strip=True)
            try:
                dt = datetime.strptime(t, "%Y-%m-%d %H:%M:%S")
            except:
                dt = None
        else:
            dt = None

        # ìš”ì•½ (mid title)
        summary_tag = soup.select_one("div.midtitle_text")
        summary = summary_tag.get_text(" ", strip=True) if summary_tag else ""

        # ë³¸ë¬¸
        parent = soup.select_one("div.news_cnt_detail_wrap")
        content_list = []

        if parent:
            for p in parent.select("p"):
                tx = p.get_text(" ", strip=True)
                if tx:
                    content_list.append(tx)

        content = "\n".join(content_list)

        return dt, summary, content

    except:
        log_error(f"MK ìƒì„¸ ì‹¤íŒ¨: {url}")
        return None, "", ""


# ----------------------------------------
# MK í¬ë¡¤ í•¨ìˆ˜
# ----------------------------------------
def crawl_mk(section, base_url, mode="today"):
    now = datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    limit_dt = now - timedelta(days=5)

    # ğŸ”¥ ì €ì¥ íŒŒì¼ëª… ë³€ê²½ (today â†’ oneday)
    save_mode = "oneday" if mode == "today" else "recent5"
    filename = f"mk_{section}_{save_mode}_{now.strftime('%Y%m%d')}.json"

    existing, existing_urls = load_existing(filename)
    print(f"\n[MK-{section}-{mode}] ê¸°ì¡´ {len(existing_urls)}ê°œ")

    new_list = []
    page_url = base_url

    while True:
        page_list, next_link = mk_fetch_list(page_url)

        for title, url in page_list:
            if url in existing_urls:
                print("â© MK ìŠ¤í‚µ:", title)
                continue

            date_dt, summary, content = mk_parse_article(url)
            if not date_dt:
                continue

            # ğŸ”¥ today í•„í„°ë§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            if mode == "today":
                if date_dt.strftime("%Y-%m-%d") != today_str:
                    continue
            else:
                if date_dt < limit_dt:
                    continue

            print("ğŸ†• MK ì‹ ê·œ:", title)

            new_list.append({
                "title": title,
                "date": date_dt.strftime("%Y-%m-%d %H:%M:%S"),
                "url": url,
                "summary": summary,
                "content": content
            })

            existing_urls.add(url)
            time.sleep(0.2)

        if not next_link:
            break
        page_url = next_link

    total = existing + new_list
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(total, f, ensure_ascii=False, indent=4)

    return total


# ----------------------------------------
# ì‹¤í–‰
# ----------------------------------------
if __name__ == "__main__":
    MK_TRADE = "https://www.mk.co.kr/news/economy/trade/"
    MK_FX = "https://www.mk.co.kr/news/economy/foreign-exchange/"

    crawl_mk("trade", MK_TRADE, "today")
    crawl_mk("trade", MK_TRADE, "recent5")

    crawl_mk("fx", MK_FX, "today")
    crawl_mk("fx", MK_FX, "recent5")
