# ============================================
# auto_qna_pipeline.py
# QnA: 1ì°¨ ë ˆë²¨ë§(SAFE/MID/HIGH) + RAG ë‹µë³€ ìƒì„±
# ê²°ê³¼ë¥¼ txtë¡œ ì €ì¥í•´ì„œ í™•ì¸ìš©
# ============================================

import os
import json
from datetime import datetime

from openai import OpenAI
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS

# -----------------------------
# 0. ì„¤ì •
# -----------------------------
VECTOR_DB_DIR = "/home/g5223sho/bnk_ai_server/auto_qna/vector_db"
LOG_DIR = "./qna_logs"

os.makedirs(LOG_DIR, exist_ok=True)

# OpenAI í´ë¼ì´ì–¸íŠ¸ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
client = OpenAI()

# LLM & Embedding (ì„ë² ë”© ëª¨ë¸ì€ ë²¡í„°DB ë§Œë“¤ ë•Œì™€ ë™ì¼í•´ì•¼ í•¨!)
EMBEDDING_MODEL_NAME = "text-embedding-3-large"  # í•„ìš”ì‹œ ìˆ˜ì •
LLM_MODEL_NAME = "gpt-4o-mini"

embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
llm = ChatOpenAI(model=LLM_MODEL_NAME, temperature=0.2)


# -----------------------------
# 1. ë²¡í„° DB ë¡œë”©
# -----------------------------
def load_vector_db():
    if not os.path.exists(VECTOR_DB_DIR):
        raise FileNotFoundError(f"ë²¡í„° DB ë””ë ‰í† ë¦¬ ì—†ìŒ: {VECTOR_DB_DIR}")
    print(f"ğŸ“‚ ë²¡í„° DB ë¡œë”©: {VECTOR_DB_DIR}")

    # langchain 0.2+ ì—ì„  allow_dangerous_deserialization=True í•„ìš”í•  ìˆ˜ ìˆìŒ
    db = FAISS.load_local(
        VECTOR_DB_DIR,
        embeddings,
        allow_dangerous_deserialization=True
    )
    return db


vector_db = load_vector_db()


# -----------------------------
# 2. ì§ˆë¬¸ ë ˆë²¨ë§ (SAFE / MID / HIGH)
# -----------------------------
def build_level_prompt(question: str, meta: dict | None = None) -> str:
    meta_text = f"\n[ë©”íƒ€ë°ì´í„°]\n{meta}\n" if meta else ""
    return f"""
ë‹¹ì‹ ì€ ì€í–‰ QnA ì§ˆë¬¸ì˜ ìœ„í—˜ë„ë¥¼ SAFE, MID, HIGH ì„¸ ë‹¨ê³„ë¡œ ë¶„ë¥˜í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

[ë ˆë²¨ ì •ì˜]
- SAFE:
  - ì˜ì—…ì‹œê°„, ì§€ì  ìœ„ì¹˜, ë‹¨ìˆœ ì ˆì°¨, ê¸°ë³¸ ìƒí’ˆ ì•ˆë‚´ ë“±
  - ê·œì œ/ë¯¼ì› ë¦¬ìŠ¤í¬ê°€ ê±°ì˜ ì—†ëŠ” ì§ˆë¬¸
- MID:
  - ì´ììœ¨, ìš°ëŒ€ê¸ˆë¦¬, ìˆ˜ìˆ˜ë£Œ, ì¤‘ë„í•´ì§€ ì¡°ê±´, í™˜ìœ¨ ì ìš© ë°©ì‹ ë“±
  - ê¸ˆìœµì •ë³´ì´ì§€ë§Œ "ìˆ˜ìµ ë³´ì¥"ì´ë‚˜ "ì†ì‹¤ ì±…ì„"ê¹Œì§€ëŠ” ì•„ë‹Œ ì§ˆë¬¸
- HIGH:
  - ì†ì‹¤/ìˆ˜ìµ ë³´ì¥, ìˆ˜ìµë¥ /í™˜ìœ¨ ìˆ˜ìµ ê³„ì‚°, íˆ¬ì ì¶”ì²œ
  - ì†ì‹¤ ì±…ì„, ë¶„ìŸ, ì†Œì†¡, ë³´ì „ ìš”êµ¬ ë“±
  - ì˜ëª» ë‹µë³€ ì‹œ ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ê°€ í° ì§ˆë¬¸

[ë‚œì´ë„]
- basic / intermediate / advanced ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒ:
  - basic: ì•„ì£¼ ë‹¨ìˆœ, ì´ˆë³´ì ìˆ˜ì¤€
  - intermediate: ì¡°ê±´/ì˜ˆì™¸ê°€ ì¼ë¶€ í¬í•¨ëœ ë³´í†µ ìˆ˜ì¤€
  - advanced: ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤, ì „ë¬¸ì ì¸ ë‚´ìš©

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ê¸ˆì§€.

{{
  "level": "SAFE" | "MID" | "HIGH",
  "complexity": "basic" | "intermediate" | "advanced",
  "reason": "ì´ ë ˆë²¨ì„ ì„ íƒí•œ ì´ìœ ë¥¼ í•œê¸€ë¡œ í•œë‘ ë¬¸ì¥",
  "tags": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]
}}

[ì§ˆë¬¸]
{question}
{meta_text}
"""


def classify_question(question: str, meta: dict | None = None) -> dict:
    prompt = build_level_prompt(question, meta)

    completion = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    raw = completion.choices[0].message.content.strip()
    print("ğŸ§© ë¶„ë¥˜ ì›ë³¸ ì‘ë‹µ:", raw)

    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        # í˜¹ì‹œ JSON í¬ë§· ì•ˆ ì§€í‚¤ë©´ ë‹¤ì‹œ í•œ ë²ˆ ê°•í•˜ê²Œ ìš”ì²­í•˜ê±°ë‚˜, fallback
        print("âš  JSON íŒŒì‹± ì‹¤íŒ¨. rawë¥¼ ê·¸ëŒ€ë¡œ level=SAFEë¡œ fallback")
        data = {
            "level": "SAFE",
            "complexity": "basic",
            "reason": f"JSON íŒŒì‹± ì‹¤íŒ¨. raw={raw}",
            "tags": []
        }

    return data


# -----------------------------
# 3. ë²¡í„° DBì—ì„œ ë¬¸ë§¥ ê²€ìƒ‰
# -----------------------------
def retrieve_context(question: str, k: int = 5) -> str:
    """
    ë²¡í„°DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ kê°œ ê²€ìƒ‰í•˜ê³ , í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    """
    docs = vector_db.similarity_search(question, k=k)
    print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ {len(docs)}ê°œ ê²€ìƒ‰")

    context_parts = []
    for i, d in enumerate(docs, start=1):
        context_parts.append(f"[ë¬¸ì„œ {i}]\n{d.page_content}\n")

    return "\n\n".join(context_parts)


# -----------------------------
# 4. ë‹µë³€ ìƒì„± (ë ˆë²¨ + ë¬¸ë§¥ í™œìš©)
# -----------------------------
def build_answer_prompt(question: str,
                        level: str,
                        complexity: str,
                        tags: list[str],
                        context: str) -> str:
    tags_text = ", ".join(tags or [])

    guardrail = ""
    if level == "MID":
        guardrail = """
- ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì¶”ì¸¡í•´ì„œ ë§í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ì•½ê´€/ìƒí’ˆì„¤ëª…ì„œ í™•ì¸ì´ í•„ìš”í•¨ì„ ì•ˆë‚´í•œë‹¤.
- ìˆ˜ìµ ë³´ì¥, ì†ì‹¤ ë¯¸ë°œìƒ ë“± í™•ì •ì ì¸ í‘œí˜„ì€ í”¼í•œë‹¤.
"""
    elif level == "HIGH":
        guardrail = """
- ìˆ˜ìµ ë³´ì¥, ì†ì‹¤ ë³´ì „, í™˜ìœ¨ ë°©í–¥ ì˜ˆì¸¡ ë“±ì„ ë‹¨ì •ì ìœ¼ë¡œ ë§í•˜ì§€ ì•ŠëŠ”ë‹¤.
- íˆ¬ì íŒë‹¨ ë° ìµœì¢… ì±…ì„ì´ ê³ ê°ì—ê²Œ ìˆë‹¤ëŠ” ì ì„ ë¶„ëª…íˆ ì•Œë¦°ë‹¤.
- êµ¬ì²´ì ì¸ íˆ¬ì ì¶”ì²œ ëŒ€ì‹ , ì¼ë°˜ì ì¸ ì›ì¹™ê³¼ ì£¼ì˜ì‚¬í•­ ìœ„ì£¼ë¡œ ì„¤ëª…í•œë‹¤.
- ë¶„ìŸ/ì±…ì„ ê´€ë ¨ íŒë‹¨ì€ í•˜ì§€ ì•Šê³ , ì •ì‹ ìƒë‹´/ë¯¼ì› ì ˆì°¨ë¡œ ì•ˆë‚´í•œë‹¤.
"""

    return f"""
ë‹¹ì‹ ì€ ì€í–‰ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]
{question}

[ì§ˆë¬¸ ë ˆë²¨]
- ìœ„í—˜ë„: {level}
- ë‚œì´ë„: {complexity}
- íƒœê·¸: {tags_text}

[ì„¤ëª… ë‚œì´ë„ ê·œì¹™]
- basic: ì–´ë ¤ìš´ ìš©ì–´ ì—†ì´ ì•„ì£¼ ì‰½ê²Œ, ì˜ˆì‹œ í¬í•¨
- intermediate: ê¸ˆìœµ ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥í•˜ë˜ ê°„ë‹¨íˆ í’€ì–´ ì„¤ëª…
- advanced: ë¹„êµì  ì „ë¬¸ì ì¸ ì„¤ëª… í—ˆìš©

[ë¦¬ìŠ¤í¬ ê°€ë“œë ˆì¼]
{guardrail}

[ì°¸ê³  ìë£Œ(RAG ì»¨í…ìŠ¤íŠ¸)]
ì•„ë˜ ë‚´ìš©ì€ ì€í–‰ ë‚´ë¶€ ê·œì •/ìƒí’ˆ ì„¤ëª…ì„œì—ì„œ ê°€ì ¸ì˜¨ ì°¸ê³ ìš© ìë£Œì…ë‹ˆë‹¤.
í•„ìš”í•œ ë‚´ìš©ë§Œ ìš”ì•½í•´ì„œ ë‹µë³€ì— ë°˜ì˜í•˜ê³ , ì§ì ‘ ë³µë¶™ì€ í”¼í•˜ì„¸ìš”.

{context}

[ì¶œë ¥ í˜•ì‹]
- ê³ ê°ì—ê²Œ ë°”ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ë‹µë³€ë§Œ ì‘ì„±
- 3~7ë¬¸ì¥, ì •ì¤‘í•œ ì¡´ëŒ“ë§ (~~ì…ë‹ˆë‹¤, ~~í•˜ì„¸ìš”)
- ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ì—†ì´, í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ë‹µë³€
"""


def generate_answer(question: str, cls: dict, context: str) -> str:
    level = cls.get("level", "SAFE")
    complexity = cls.get("complexity", "basic")
    tags = cls.get("tags", [])

    prompt = build_answer_prompt(
        question=question,
        level=level,
        complexity=complexity,
        tags=tags,
        context=context
    )

    completion = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )

    answer = completion.choices[0].message.content.strip()
    return answer


# -----------------------------
# 5. ê²°ê³¼ë¥¼ TXTë¡œ ì €ì¥
# -----------------------------
def save_result_to_txt(question: str, cls: dict, context: str, answer: str) -> str:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(LOG_DIR, f"qna_result_{ts}.txt")

    with open(filename, "w", encoding="utf-8") as f:
        f.write("=== QNA AUTO PIPELINE ê²°ê³¼ ===\n\n")
        f.write("[ì§ˆë¬¸]\n")
        f.write(question + "\n\n")

        f.write("[ë¶„ë¥˜ ê²°ê³¼]\n")
        f.write(json.dumps(cls, ensure_ascii=False, indent=2) + "\n\n")

        f.write("[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½]\n")
        f.write(context + "\n\n")

        f.write("[ìµœì¢… ë‹µë³€]\n")
        f.write(answer + "\n")

    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")
    return filename


# -----------------------------
# 6. ë©”ì¸ ë£¨í”„ (í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸)
# -----------------------------
def run_interactive():
    print("=== QnA AUTO (ë ˆë²¨ë§ + ë‹µë³€ìƒì„±) í…ŒìŠ¤íŠ¸ ===")
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ë¹ˆ ì¤„ ì…ë ¥ ì‹œ ì¢…ë£Œ.\n")

    while True:
        question = input("Q> ").strip()
        if not question:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 1) ë ˆë²¨ë§
        cls = classify_question(question)
        print("\nğŸ§· ë¶„ë¥˜ ê²°ê³¼:", cls, "\n")

        # 2) ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context = retrieve_context(question, k=5)

        # 3) ë‹µë³€ ìƒì„±
        answer = generate_answer(question, cls, context)
        print("\nğŸ’¬ ìƒì„±ëœ ë‹µë³€:\n", answer, "\n")

        # 4) TXT ì €ì¥
        save_result_to_txt(question, cls, context, answer)
        print("=" * 60 + "\n")


if __name__ == "__main__":
    run_interactive()
