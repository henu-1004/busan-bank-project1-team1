# /home/g5223sho/bnk_ai_server/auto_qna/answer.py
# ============================================
# QnA ë™ì‘ ë¡œì§ ëª¨ë“ˆ
#  - ë²¡í„°DB ë¡œë”©
#  - ì§ˆë¬¸ ë ˆë²¨ë§ (SAFE / MID / HIGH)
#  - ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (RAG)
#  - ë‹µë³€ ìƒì„±
#  - TB_QNA ì—…ë°ì´íŠ¸ (SAFE / MID / HIGH)
#  - MID/HIGH: draft ì•ì— "AI ìƒì„± ì´ˆì•ˆì…ë‹ˆë‹¤. ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤." ê³ ì •ë¬¸êµ¬ ì¶”ê°€
#  - ë ˆë²¨ë§/ë‹µë³€ ìƒì„±ì€ ì œëª© + ë‚´ìš©ì„ í•¨ê»˜ ì‚¬ìš©
# ============================================

import os
import json
from typing import Optional, List, Dict, Any

from openai import OpenAI
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS

from qna_db import update_qna_safe, update_qna_mid_high

# -----------------------------
# 0. ê²½ë¡œ/ëª¨ë¸ ì„¤ì •
# -----------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))      # /home/.../auto_qna
VECTOR_DB_DIR = os.path.join(BASE_DIR, "vector_db")        # ./vector_db

EMBEDDING_MODEL_NAME = "text-embedding-3-large"
LLM_MODEL_NAME = "gpt-4o-mini"

client = OpenAI()
embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
llm = ChatOpenAI(model=LLM_MODEL_NAME, temperature=0.2)


# -----------------------------
# 1. ë²¡í„° DB ë¡œë”©
# -----------------------------
def load_vector_db():
    if not os.path.exists(VECTOR_DB_DIR):
        raise FileNotFoundError(f"ë²¡í„° DB ë””ë ‰í† ë¦¬ ì—†ìŒ: {VECTOR_DB_DIR}")
    print(f"ğŸ“‚ QnA ë²¡í„° DB ë¡œë”©: {VECTOR_DB_DIR}")

    db = FAISS.load_local(
        VECTOR_DB_DIR,
        embeddings,
        allow_dangerous_deserialization=True,
    )
    return db


vector_db = load_vector_db()


# -----------------------------
# 2. ì§ˆë¬¸ ë ˆë²¨ë§ (SAFE / MID / HIGH)
# -----------------------------
def build_level_prompt(question: str, meta: Optional[dict] = None) -> str:
    meta_text = f"\n[ë©”íƒ€ë°ì´í„°]\n{meta}\n" if meta else ""
    return f"""
ë‹¹ì‹ ì€ ì€í–‰ QnA ì§ˆë¬¸ì˜ ìœ„í—˜ë„ë¥¼ SAFE, MID, HIGH ì„¸ ë‹¨ê³„ë¡œ ë¶„ë¥˜í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

[ë ˆë²¨ ì •ì˜]
- SAFE:
  - ì˜ì—…ì‹œê°„, ì§€ì  ìœ„ì¹˜, ë‹¨ìˆœ ì ˆì°¨, ê¸°ë³¸ ìƒí’ˆ ì•ˆë‚´ ë“±
  - ê·œì œ/ë¯¼ì› ë¦¬ìŠ¤í¬ê°€ ê±°ì˜ ì—†ëŠ” ì§ˆë¬¸
- MID:
  - ì´ììœ¨, ìš°ëŒ€ê¸ˆë¦¬, ìˆ˜ìˆ˜ë£Œ, ì¤‘ë„í•´ì§€ ì¡°ê±´, í™˜ìœ¨ ì ìš© ë°©ì‹ ë“±
  - ê¸ˆìœµì •ë³´ì´ì§€ë§Œ "ìˆ˜ìµ ë³´ì¥"ì´ë‚˜ "ì†ì‹¤ ì±…ì„"ê¹Œì§€ëŠ” ì•„ë‹Œ ì§ˆë¬¸
- HIGH:
  - ì†ì‹¤/ìˆ˜ìµ ë³´ì¥, ìˆ˜ìµë¥ /í™˜ìœ¨ ìˆ˜ìµ ê³„ì‚°, íˆ¬ì ì¶”ì²œ
  - ì†ì‹¤ ì±…ì„, ë¶„ìŸ, ì†Œì†¡, ë³´ì „ ìš”êµ¬ ë“±
  - ì˜ëª» ë‹µë³€ ì‹œ ë²•ì /ê·œì œ ë¦¬ìŠ¤í¬ê°€ í° ì§ˆë¬¸

[ë‚œì´ë„]
- basic / intermediate / advanced ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒ:
  - basic: ì•„ì£¼ ë‹¨ìˆœ, ì´ˆë³´ì ìˆ˜ì¤€
  - intermediate: ì¡°ê±´/ì˜ˆì™¸ê°€ ì¼ë¶€ í¬í•¨ëœ ë³´í†µ ìˆ˜ì¤€
  - advanced: ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤, ì „ë¬¸ì ì¸ ë‚´ìš©

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ê¸ˆì§€.

{{
  "level": "SAFE" | "MID" | "HIGH",
  "complexity": "basic" | "intermediate" | "advanced",
  "reason": "ì´ ë ˆë²¨ì„ ì„ íƒí•œ ì´ìœ ë¥¼ í•œê¸€ë¡œ í•œë‘ ë¬¸ì¥",
  "tags": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]
}}

[ì§ˆë¬¸]
{question}
{meta_text}
"""


def classify_question(question: str, meta: Optional[dict] = None) -> dict:
    prompt = build_level_prompt(question, meta)

    completion = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )

    raw = completion.choices[0].message.content.strip()
    print("ğŸ§© QnA ë¶„ë¥˜ ì›ë³¸ ì‘ë‹µ:", raw)

    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        print("âš  JSON íŒŒì‹± ì‹¤íŒ¨. SAFE/basic ë¡œ fallback")
        data = {
            "level": "SAFE",
            "complexity": "basic",
            "reason": f"JSON íŒŒì‹± ì‹¤íŒ¨: raw={raw}",
            "tags": [],
        }

    return data


# -----------------------------
# 3. ë²¡í„° DBì—ì„œ ë¬¸ë§¥ ê²€ìƒ‰
# -----------------------------
def retrieve_context(question: str, k: int = 5) -> str:
    docs = vector_db.similarity_search(question, k=k)
    print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ {len(docs)}ê°œ ê²€ìƒ‰")

    parts: List[str] = []
    for i, d in enumerate(docs, start=1):
        parts.append(f"[ë¬¸ì„œ {i}]\n{d.page_content}\n")

    return "\n\n".join(parts)


# -----------------------------
# 4. ë‹µë³€ ìƒì„±
# -----------------------------
def build_answer_prompt(question: str,
                        level: str,
                        complexity: str,
                        tags,
                        context: str) -> str:
    tags_text = ", ".join(tags or [])

    guardrail = ""
    if level == "MID":
        guardrail = """
- ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì¶”ì¸¡í•´ì„œ ë§í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ì•½ê´€/ìƒí’ˆì„¤ëª…ì„œ í™•ì¸ì´ í•„ìš”í•¨ì„ ì•ˆë‚´í•œë‹¤.
- ìˆ˜ìµ ë³´ì¥, ì†ì‹¤ ë¯¸ë°œìƒ ë“± í™•ì •ì ì¸ í‘œí˜„ì€ í”¼í•œë‹¤.
"""
    elif level == "HIGH":
        guardrail = """
- ìˆ˜ìµ ë³´ì¥, ì†ì‹¤ ë³´ì „, í™˜ìœ¨ ë°©í–¥ ì˜ˆì¸¡ ë“±ì„ ë‹¨ì •ì ìœ¼ë¡œ ë§í•˜ì§€ ì•ŠëŠ”ë‹¤.
- íˆ¬ì íŒë‹¨ ë° ìµœì¢… ì±…ì„ì´ ê³ ê°ì—ê²Œ ìˆë‹¤ëŠ” ì ì„ ë¶„ëª…íˆ ì•Œë¦°ë‹¤.
- êµ¬ì²´ì ì¸ íˆ¬ì ì¶”ì²œ ëŒ€ì‹ , ì¼ë°˜ì ì¸ ì›ì¹™ê³¼ ì£¼ì˜ì‚¬í•­ ìœ„ì£¼ë¡œ ì„¤ëª…í•œë‹¤.
- ë¶„ìŸ/ì±…ì„ ê´€ë ¨ íŒë‹¨ì€ í•˜ì§€ ì•Šê³ , ì •ì‹ ìƒë‹´/ë¯¼ì› ì ˆì°¨ë¡œ ì•ˆë‚´í•œë‹¤.
"""

    return f"""
ë‹¹ì‹ ì€ ì€í–‰ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]
{question}

[ì§ˆë¬¸ ë ˆë²¨]
- ìœ„í—˜ë„: {level}
- ë‚œì´ë„: {complexity}
- íƒœê·¸: {tags_text}

[ì„¤ëª… ë‚œì´ë„ ê·œì¹™]
- basic: ì–´ë ¤ìš´ ìš©ì–´ ì—†ì´ ì•„ì£¼ ì‰½ê²Œ, ì˜ˆì‹œ í¬í•¨
- intermediate: ê¸ˆìœµ ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥í•˜ë˜ ê°„ë‹¨íˆ í’€ì–´ ì„¤ëª…
- advanced: ë¹„êµì  ì „ë¬¸ì ì¸ ì„¤ëª… í—ˆìš©

[ë¦¬ìŠ¤í¬ ê°€ë“œë ˆì¼]
{guardrail}

[ì°¸ê³  ìë£Œ (RAG ì»¨í…ìŠ¤íŠ¸)]
ì•„ë˜ ë‚´ìš©ì€ ì€í–‰ ë‚´ë¶€ ê·œì •/ìƒí’ˆ ì„¤ëª…ì„œ ë“±ì—ì„œ ê°€ì ¸ì˜¨ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤.
í•„ìš”í•œ ë‚´ìš©ë§Œ ìš”ì•½í•´ì„œ ë‹µë³€ì— ë°˜ì˜í•˜ê³ , ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì§€ëŠ” ë§ˆì„¸ìš”.

{context}

[ì¶œë ¥ í˜•ì‹]
- ê³ ê°ì—ê²Œ ë°”ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ë‹µë³€ë§Œ ì‘ì„±
- 3~7ë¬¸ì¥, ì •ì¤‘í•œ ì¡´ëŒ“ë§ (~~ì…ë‹ˆë‹¤, ~~í•˜ì„¸ìš”)
- ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ì—†ì´, í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ë‹µë³€
"""


def generate_answer(question: str, cls: dict, context: str) -> str:
    level = cls.get("level", "SAFE")
    complexity = cls.get("complexity", "basic")
    tags = cls.get("tags", [])

    prompt = build_answer_prompt(
        question=question,
        level=level,
        complexity=complexity,
        tags=tags,
        context=context,
    )

    completion = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )

    answer = completion.choices[0].message.content.strip()
    return answer


# -----------------------------
# 5. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
#    (FastAPIì—ì„œëŠ” ì´ í•¨ìˆ˜ë§Œ í˜¸ì¶œ)
# -----------------------------
def process_qna(qna_no: int,
                question: str,
                title: Optional[str] = None,
                meta: Optional[dict] = None) -> Dict[str, Any]:
    """
    QnA í•˜ë‚˜ì— ëŒ€í•´:
    - (ì œëª© + ë‚´ìš©) ê¸°ë°˜ìœ¼ë¡œ ë ˆë²¨ë§
    - (ì œëª© + ë‚´ìš©) ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    - (ì œëª© + ë‚´ìš©) ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    - MID/HIGHì¼ ë•Œ ë“œë˜í”„íŠ¸ ì•ì— ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
    - TB_QNA (QNA_DRAFT, QNA_REPLY, QNA_STATUS) ì—…ë°ì´íŠ¸
    """
    print(f"ğŸš€ QnA ì²˜ë¦¬ ì‹œì‘ (answer.py): qnaNo={qna_no}")

    # ğŸ”¹ ì œëª© + ë‚´ìš©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    if title:
        full_question = f"[ì œëª©]\n{title}\n\n[ë‚´ìš©]\n{question}"
    else:
        full_question = question

    # 1) ë ˆë²¨ë§ (ì œëª©+ë‚´ìš© ê¸°ì¤€)
    cls = classify_question(full_question, meta)

    # 2) ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì œëª©+ë‚´ìš© ê¸°ì¤€)
    context = retrieve_context(full_question, k=5)

    # 3) ë‹µë³€ ìƒì„± (ì œëª©+ë‚´ìš© ê¸°ì¤€)
    answer = generate_answer(full_question, cls, context)

    # 4) ë“œë˜í”„íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
    level = cls.get("level", "SAFE")
    draft = answer

    # MID/HIGHë©´ ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ë°˜ë“œì‹œ ì•ì— ë¶™ì„
    if level in ("MID", "HIGH"):
        prefix = "AI ìƒì„± ì´ˆì•ˆì…ë‹ˆë‹¤. ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n"
        draft = prefix + answer

    # 5) DB ì—…ë°ì´íŠ¸ (TB_QNA)
    try:
        if level == "SAFE":
            # SAFE: ì´ˆì•ˆì€ draft, ìœ ì €ì—ê²Œ ë°”ë¡œ ë³´ì—¬ì¤„ replyëŠ” answer
            update_qna_safe(qna_no, answer=answer, draft=draft)
        elif level in ("MID", "HIGH"):
            # MID/HIGH: replyëŠ” ë¹„ì›Œë‘ê³  draft + statusë§Œ ì„¤ì •
            update_qna_mid_high(qna_no, draft=draft, level=level)
        else:
            # ì´ìƒí•œ ê°’ì´ë©´ ì¼ë‹¨ SAFEì²˜ëŸ¼ ì²˜ë¦¬
            print(f"âš  ì•Œ ìˆ˜ ì—†ëŠ” level({level}), SAFEë¡œ ì²˜ë¦¬: qnaNo={qna_no}")
            update_qna_safe(qna_no, answer=answer, draft=draft)
    except Exception as e:
        print(f"âŒ TB_QNA ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: qnaNo={qna_no}, error={e}")

    result: Dict[str, Any] = {
        "qnaNo": qna_no,
        "level": level,
        "complexity": cls.get("complexity", "basic"),
        "reason": cls.get("reason", ""),
        "tags": cls.get("tags", []),
        "draft": draft,
        "answer": answer,
    }

    print(f"âœ… QnA ì²˜ë¦¬ ì™„ë£Œ (answer.py): {result}")
    return result
